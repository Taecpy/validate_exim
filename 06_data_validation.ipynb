{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = [\n",
    "    \"RequestID\",\n",
    "    \"RatingYear\",\n",
    "    \"WorkFlowStepName\",\n",
    "    \"CustomerRefID\",\n",
    "    \"ScoreModel\",\n",
    "    \"FlowModelExisting\",\n",
    "    \"FlowModelNew62\",\n",
    "]\n",
    "\n",
    "date_columns = [\n",
    "    \"RequestDate\",\n",
    "    \"RatingDate\"\n",
    "]\n",
    "\n",
    "numeric_columns = [\"adjCompositeScore\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_file(file_path: str, **kwargs) -> pd.DataFrame:\n",
    "    try:\n",
    "        file_extension = file_path.split(\".\")[-1].lower()\n",
    "\n",
    "        if file_extension == \"csv\":\n",
    "            df = pd.read_csv(file_path, **kwargs)\n",
    "        elif file_extension == \"xlsx\":\n",
    "            df = pd.read_excel(file_path, **kwargs)\n",
    "        elif file_extension == \"parquet\":\n",
    "            df = pd.read_parquet(file_path, **kwargs)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file extension: {file_extension}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {file_path} does not exist.\")\n",
    "        raise\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f\"Error: Parsing error for file {file_path} - {str(e)}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occured: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "def import_multiple_files(\n",
    "    directory_path: str, extension: str, **kwargs\n",
    ") -> pd.DataFrame:\n",
    "    files = [f for f in os.listdir(directory_path) if f.endswith(f\".{extension}\")]\n",
    "    dataframes = []\n",
    "    row_counts = {}\n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory_path, file)\n",
    "\n",
    "        if extension == \"csv\":\n",
    "            df = pd.read_csv(file_path, **kwargs)\n",
    "            columns = [c for c in df.columns if c.lower()[:7] != \"unnamed\"]\n",
    "            df = df[columns]\n",
    "        elif extension == \"xlsx\":\n",
    "            df = pd.read_excel(file_path, **kwargs)\n",
    "        elif extension == \"parquet\":\n",
    "            df = pd.read_parquet(file_path, **kwargs)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupport file extension: {extension}\")\n",
    "        row_counts[file] = len(df)\n",
    "        dataframes.append(df)\n",
    "\n",
    "    concatenate_df = pd.concat(dataframes, ignore_index=True)\n",
    "    row_counts[\"total\"] = len(concatenate_df)\n",
    "\n",
    "    return concatenate_df\n",
    "\n",
    "def convert_to_date(\n",
    "    df: pd.DataFrame, column_names: list[str], date_format: str = \"%Y-%m-%d\"\n",
    ") -> pd.DataFrame:\n",
    "    for column_name in column_names:\n",
    "        if column_name not in df.columns:\n",
    "            raise ValueError(\n",
    "                f\"The column '{column_name}' does not exist in the DataFrame.\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            df[column_name] = pd.to_datetime(\n",
    "                df[column_name], format=date_format, errors=\"coerce\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error converting column '{column_name}' to date: {e}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def categorical_columns_values(df: pd.DataFrame, str_columns: list[str], max_examples: int=10) -> None:\n",
    "    categorical_columns = str_columns\n",
    "    stats = {}\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        unique_values = df[col].unique()\n",
    "        unique_count = len(unique_values)\n",
    "        if unique_count > max_examples:\n",
    "            example_values = unique_values[:max_examples].tolist()\n",
    "        else:\n",
    "            example_values = unique_values.tolist()\n",
    "            \n",
    "        stats[col] = {\n",
    "            'unique_count': unique_count,\n",
    "            'example_values': example_values\n",
    "        }\n",
    "    \n",
    "    return  stats\n",
    "        \n",
    "def date_columns_stats(df: pd.DataFrame, date_columns=date_columns) -> dict:\n",
    "    date_columns = date_columns\n",
    "    stats = {}\n",
    "    \n",
    "    for col in date_columns:\n",
    "        min_date = df[col].min()\n",
    "        max_date = df[col].max()\n",
    "        null_count = df[col].isnull().sum()\n",
    "        stats[col] = {'min': min_date, 'max': max_date, 'null_count': null_count}\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def numeric_columns_outlier(df: pd.DataFrame, numeric_columns: list[str]) -> dict:\n",
    "    numeric_columns = numeric_columns\n",
    "    stats = {}\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)        \n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = max(Q1 - 1.5 * IQR, 0)\n",
    "        upper_bound = min(Q3 + 1.5 * IQR, 100)\n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\n",
    "        outlier_count = outliers.count()\n",
    "        \n",
    "        stats[col] = {\n",
    "            'Q1': Q1,\n",
    "            'Q3': Q3,\n",
    "            'IQR': IQR,\n",
    "            'lower_bound': lower_bound,\n",
    "            'upper_bound': upper_bound,\n",
    "            'outlier_count': outlier_count,\n",
    "        }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def check_duplicates(df: pd.DataFrame) -> None:\n",
    "    duplicates = df[df.duplicated(keep=False)]\n",
    "    duplicate_count = duplicates.shape[0]\n",
    "    \n",
    "    return duplicate_count, duplicates\n",
    "\n",
    "def check_missing_values(df: pd.DataFrame):\n",
    "    missing_values = df.isnull().sum()\n",
    "    total_values = len(df)\n",
    "    missing_percentage = (missing_values / total_values) * 100\n",
    "    \n",
    "    stats = pd.DataFrame({\n",
    "        'missing_values': missing_values,\n",
    "        'missing_percentage': missing_percentage\n",
    "    }).sort_values(by='missing_values', ascending=False)\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def create_boxplot(df: pd.DataFrame, column: str) -> None:\n",
    "    col = column\n",
    "    \n",
    "    plt.figure(figsize=(10, 2))\n",
    "    sns.boxplot(x=df[col], showfliers=True)\n",
    "    plt.title(f'Boxplot for {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.show\n",
    "\n",
    "def plot_monthly_observation(df: pd.DataFrame, countby=str) -> None:\n",
    "    \n",
    "    if \"RatingDate\" not in df.columns or f\"{countby}\" not in df.columns:\n",
    "        raise ValueError(f\"DataFrame must contain 'RatingDate' and {countby} columns\")\n",
    "    \n",
    "    df['year_month'] = df[\"RatingDate\"].dt.to_period('M')\n",
    "    \n",
    "    monthly_counts = df.groupby('year_month').size()\n",
    "    \n",
    "    plt.figure(figsize=(10, 3))\n",
    "    monthly_counts.plot(kind='bar')\n",
    "    plt.title('Monthly Observation')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Number of Observation')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def create_data_by_time(df: pd.DataFrame, by: str = 'M') -> pd.DataFrame:\n",
    "    if \"RatingDate\" not in df.columns:\n",
    "        raise ValueError(f\"DataFrame must contain 'RatingDate' columns\")\n",
    "    \n",
    "    df['YearMonth'] = df[\"RatingDate\"].dt.to_period(f'{by}')\n",
    "    monthly_counts = df.groupby('YearMonth').size().reset_index(name='Count')\n",
    "    \n",
    "    return monthly_counts\n",
    "\n",
    "def adj_rsme_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    adj_df = df.copy()\n",
    "    adj_df[\"RequestID\"] = np.where(df[\"RequestID\"].isnull(), df[\"รหัสเอกสาร\"], df[\"RequestID\"])\n",
    "    return adj_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corporate_path = '../data/processed/02_data_sampling/corporate_customer_data.parquet'\n",
    "sme_path =  '../data/processed/02_data_sampling/sme_customer_data.parquet'\n",
    "rsme_path = \"../data/processed/02_data_sampling/Retail_202005_202403.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corporate_df = import_file(file_path=corporate_path)\n",
    "sme_df = import_file(file_path=sme_path)\n",
    "rsme_df_tmp = import_file(file_path=rsme_path)\n",
    "rsme_df = adj_rsme_df(rsme_df_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsme_df.dtypes.reset_index().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Corporate Portfolio:\")\n",
    "duplicate_count_corp, duplicates_corp = check_duplicates(corporate_df)\n",
    "\n",
    "print(f\" Total Observation: \" + str(corporate_df.shape[0]))\n",
    "print(f\" Number of duplicate rows: {duplicate_count_corp}\")\n",
    "print(f\" Unique RequestID: \" + str(len(corporate_df['RequestID'].unique())))\n",
    "if duplicate_count_corp > 0:\n",
    "    print(\"Duplicate rows:\")\n",
    "    print(duplicates_corp)\n",
    "print()\n",
    "\n",
    "print(\"SMEs Portfolio:\")\n",
    "duplicate_count_sme, duplicates_sme = check_duplicates(sme_df)\n",
    "\n",
    "print(f\" Total Observation: \" + str(sme_df.shape[0]))\n",
    "print(f\" Number of duplicate rows: {duplicate_count_sme}\")\n",
    "print(f\" Unique RequestID: \" + str(len(sme_df['RequestID'].unique())))\n",
    "if duplicate_count_sme > 0:\n",
    "    print(\"Duplicate rows:\")\n",
    "    print(duplicates_sme)\n",
    "print()\n",
    "\n",
    "print(\"Retail SMEs Portfolio:\")\n",
    "duplicate_count_rsme, duplicates_rsme = check_duplicates(rsme_df)\n",
    "\n",
    "print(f\" Total Observation: \" + str(rsme_df.shape[0]))\n",
    "print(f\" Number of duplicate rows: {duplicate_count_rsme}\")\n",
    "print(f\" Unique RequestID: \" + str(len(rsme_df['RequestID'].unique())))\n",
    "if duplicate_count_rsme > 0:\n",
    "    print(\"Duplicate rows:\")\n",
    "    print(duplicates_rsme)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category Field Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corporate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_stats = categorical_columns_values(corporate_df, categorical_columns)\n",
    "\n",
    "for col, stat in categorical_stats.items():\n",
    "    print(f'Column: {col}')\n",
    "    print(f' Unique Count: {stat['unique_count']}')\n",
    "    print(f' Example Values: {stat['example_values']}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_stats = check_missing_values(corporate_df[categorical_columns])\n",
    "print(missing_value_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_stats = categorical_columns_values(sme_df, categorical_columns)\n",
    "\n",
    "for col, stat in categorical_stats.items():\n",
    "    print(f'Column: {col}')\n",
    "    print(f' Unique Count: {stat['unique_count']}')\n",
    "    print(f' Example Values: {stat['example_values']}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_stats = check_missing_values(sme_df[categorical_columns])\n",
    "print(missing_value_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retail SMEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_stats = categorical_columns_values(rsme_df, categorical_columns)\n",
    "\n",
    "for col, stat in categorical_stats.items():\n",
    "    print(f'Column: {col}')\n",
    "    print(f' Unique Count: {stat['unique_count']}')\n",
    "    print(f' Example Values: {stat['example_values']}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_stats = check_missing_values(rsme_df[categorical_columns])\n",
    "print(missing_value_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Field Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corporate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_stats = date_columns_stats(df=corporate_df)\n",
    "\n",
    "for col, stat in date_stats.items():\n",
    "    print(f\"Column: {col}\")\n",
    "    print(f\" Min: {stat['min']}\")\n",
    "    print(f\" Max: {stat['max']}\")\n",
    "    print(f\" Count Unique: {len(corporate_df[col].unique())}\")\n",
    "    print(f\" Null Count: {stat['null_count']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_stats = date_columns_stats(df=sme_df)\n",
    "\n",
    "for col, stat in date_stats.items():\n",
    "    print(f\"Column: {col}\")\n",
    "    print(f\" Min: {stat['min']}\")\n",
    "    print(f\" Max: {stat['max']}\")\n",
    "    print(f\" Count Unique: {len(sme_df[col].unique())}\")\n",
    "    print(f\" Null Count: {stat['null_count']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retail SMEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_stats = date_columns_stats(df=rsme_df)\n",
    "\n",
    "for col, stat in date_stats.items():\n",
    "    print(f\"Column: {col}\")\n",
    "    print(f\" Min: {stat['min']}\")\n",
    "    print(f\" Max: {stat['max']}\")\n",
    "    print(f\" Count Unique: {len(rsme_df[col].unique())}\")\n",
    "    print(f\" Null Count: {stat['null_count']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Field Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_stats = numeric_columns_outlier(corporate_df, numeric_columns=numeric_columns)\n",
    "\n",
    "for col, stat in outlier_stats.items():\n",
    "    print(f'Column: {col}')\n",
    "    print(f' Min: {corporate_df[\"adjCompositeScore\"].min()}')\n",
    "    print(f' Q1: {stat['Q1']}')\n",
    "    print(f' Q3: {stat['Q3']}')\n",
    "    print(f' Max: {corporate_df[\"adjCompositeScore\"].max()}')\n",
    "    print(f' IQR: {stat['IQR']}')\n",
    "    print(f' Lower Bound: {stat['lower_bound']}')\n",
    "    print(f' Upper Bound: {stat['upper_bound']}')\n",
    "    print(f' Outlier Count: {stat['outlier_count']}')\n",
    "    print()\n",
    "\n",
    "print(\"Missing Value: \" + str(corporate_df[\"adjCompositeScore\"].isnull().sum()))\n",
    "print(\"Count Unique: \" + str(len(corporate_df[\"adjCompositeScore\"].unique())))\n",
    "print(f\"{corporate_df[\"adjCompositeScore\"].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_stats = numeric_columns_outlier(sme_df, numeric_columns=numeric_columns)\n",
    "\n",
    "for col, stat in outlier_stats.items():\n",
    "    print(f'Column: {col}')\n",
    "    print(f' Min: {sme_df[\"adjCompositeScore\"].min()}')\n",
    "    print(f' Q1: {stat['Q1']}')\n",
    "    print(f' Q3: {stat['Q3']}')\n",
    "    print(f' Max: {sme_df[\"adjCompositeScore\"].max()}')\n",
    "    print(f' IQR: {stat['IQR']}')\n",
    "    print(f' Lower Bound: {stat['lower_bound']}')\n",
    "    print(f' Upper Bound: {stat['upper_bound']}')\n",
    "    print(f' Outlier Count: {stat['outlier_count']}')\n",
    "    print()\n",
    "\n",
    "print(\"Missing Value: \" + str(sme_df[\"adjCompositeScore\"].isnull().sum()))\n",
    "print(\"Count Unique: \" + str(len(sme_df[\"adjCompositeScore\"].unique())))\n",
    "print(f\"{sme_df[\"adjCompositeScore\"].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_stats = numeric_columns_outlier(rsme_df, numeric_columns=numeric_columns)\n",
    "\n",
    "for col, stat in outlier_stats.items():\n",
    "    print(f'Column: {col}')\n",
    "    print(f' Min: {rsme_df[\"adjCompositeScore\"].min()}')\n",
    "    print(f' Q1: {stat['Q1']}')\n",
    "    print(f' Q3: {stat['Q3']}')\n",
    "    print(f' Max: {rsme_df[\"adjCompositeScore\"].max()}')\n",
    "    print(f' IQR: {stat['IQR']}')\n",
    "    print(f' Lower Bound: {stat['lower_bound']}')\n",
    "    print(f' Upper Bound: {stat['upper_bound']}')\n",
    "    print(f' Outlier Count: {stat['outlier_count']}')\n",
    "    print()\n",
    "\n",
    "print(\"Missing Value: \" + str(rsme_df[\"adjCompositeScore\"].isnull().sum()))\n",
    "print(\"Count Unique: \" + str(len(rsme_df[\"adjCompositeScore\"].unique())))\n",
    "print(f\"{rsme_df[\"adjCompositeScore\"].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summation Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_equality(df: pd.DataFrame, column1: str, column2: str, precision=2) -> None:\n",
    "    \n",
    "    rounded_col1 = df[column1].round(precision)\n",
    "    rounded_col2 = df[column2].round(precision)\n",
    "    \n",
    "    equality = rounded_col1 == rounded_col2\n",
    "    \n",
    "    total_count = len(df)\n",
    "    equal_count = equality.sum()\n",
    "    unequal_count = total_count - equal_count\n",
    "    \n",
    "    print(f\"Total values compared: {total_count}\")\n",
    "    print(f\"Number of equal values: {equal_count}\")\n",
    "    print(f\"Number of unequal values: {unequal_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sum_score_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    score_columns = [\n",
    "        \"CompositeScore\",\n",
    "        \"adjFinancialScore\",\n",
    "        \"BusinessScore\",\n",
    "        \"IndustryScore\"\n",
    "    ]\n",
    "    \n",
    "    df_1 = df[score_columns].copy()\n",
    "    sumScore = (df_1[\"adjFinancialScore\"] + df_1[\"BusinessScore\"] + df_1[\"IndustryScore\"])\n",
    "    # df_1[\"sumScore\"] = np.where(sumScore > 100, sumScore / 10, sumScore)\n",
    "    df_1[\"sumScore\"] = sumScore\n",
    "    \n",
    "    return df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Corporate Portfolio: \")\n",
    "check_equality(create_sum_score_column(corporate_df), \"CompositeScore\", \"sumScore\")\n",
    "print()\n",
    "print(\"SMEs Portfolio: \")\n",
    "check_equality(create_sum_score_column(sme_df), \"CompositeScore\", \"sumScore\")\n",
    "print()\n",
    "print(\"RSMEs Portfolio:\")\n",
    "check_equality(create_sum_score_column(rsme_df), \"CompositeScore\", \"sumScore\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
